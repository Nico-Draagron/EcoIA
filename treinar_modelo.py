import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader, random_split
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os

# üìÇ Configura√ß√µes do projeto
data_dir = r'C:\Users\usuario\Desktop\projetos\Oikos\dataset\dataset-resized\dataset-resized'
model_path = "modelo_oikos.pt"
batch_size = 16
learning_rate = 0.001
num_epochs = 10

# üßπ Transforma√ß√µes para treino (com data augmentation)
train_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(p=0.5),  # Augmentation
    transforms.RandomRotation(10),            # Augmentation
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225])
])

# üßπ Transforma√ß√µes para teste (sem augmentation)
test_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225])
])

def prepare_data():
    """
    Prepara e divide os dados em treino e teste (80/20)
    """
    print("üì• Carregando dataset...")
    
    # Carregar dataset completo
    full_dataset = datasets.ImageFolder(root=data_dir, transform=train_transform)
    
    # Calcular tamanhos para split 80/20
    total_size = len(full_dataset)
    train_size = int(0.8 * total_size)
    test_size = total_size - train_size
    
    # Dividir dataset
    train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])
    
    # Aplicar transforma√ß√µes espec√≠ficas para teste
    test_dataset.dataset = datasets.ImageFolder(root=data_dir, transform=test_transform)
    
    # Criar DataLoaders
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    
    print(f"‚úÖ Dataset carregado:")
    print(f"   - Total de amostras: {total_size}")
    print(f"   - Treino: {train_size} amostras")
    print(f"   - Teste: {test_size} amostras")
    print(f"   - Classes: {full_dataset.classes}")
    
    return train_loader, test_loader, full_dataset.classes

def create_model(num_classes):
    """
    Cria o modelo EfficientNetB0 com transfer learning
    """
    print("üß† Configurando modelo EfficientNetB0...")
    
    # Carregar modelo pr√©-treinado
    model = models.efficientnet_b0(pretrained=True)
    
    # Congelar layers iniciais para transfer learning
    for param in model.features.parameters():
        param.requires_grad = False
    
    # Substituir classificador final
    model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)
    
    print(f"‚úÖ Modelo configurado para {num_classes} classes")
    return model

def train_model(model, train_loader, criterion, optimizer, device):
    """
    Treina o modelo e retorna hist√≥rico de loss
    """
    model.train()
    train_losses = []
    
    print("üöÄ Iniciando treinamento...")
    
    for epoch in range(num_epochs):
        running_loss = 0.0
        correct_predictions = 0
        total_samples = 0
        
        for batch_idx, (inputs, labels) in enumerate(train_loader):
            inputs, labels = inputs.to(device), labels.to(device)
            
            # Forward pass
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            
            # Backward pass
            loss.backward()
            optimizer.step()
            
            # Estat√≠sticas
            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total_samples += labels.size(0)
            correct_predictions += (predicted == labels).sum().item()
            
            # Log a cada 10 batches
            if batch_idx % 10 == 0:
                print(f"   Batch [{batch_idx}/{len(train_loader)}] - Loss: {loss.item():.4f}")
        
        # M√©tricas da √©poca
        epoch_loss = running_loss / len(train_loader)
        epoch_acc = 100 * correct_predictions / total_samples
        train_losses.append(epoch_loss)
        
        print(f"√âpoca [{epoch+1}/{num_epochs}]:")
        print(f"   - Loss: {epoch_loss:.4f}")
        print(f"   - Acur√°cia Treino: {epoch_acc:.2f}%\n")
    
    return train_losses

def evaluate_model(model, test_loader, classes, device):
    """
    Avalia o modelo no conjunto de teste e calcula m√©tricas
    """
    model.eval()
    all_predictions = []
    all_labels = []
    
    print("üîç Avaliando modelo no conjunto de teste...")
    
    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            
            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    # Calcular m√©tricas
    accuracy = accuracy_score(all_labels, all_predictions)
    f1 = f1_score(all_labels, all_predictions, average='weighted')
    
    print("üìä Resultados da Avalia√ß√£o:")
    print(f"   - Acur√°cia: {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"   - F1-Score: {f1:.4f}")
    
    # Relat√≥rio detalhado
    print("\nüìã Relat√≥rio de Classifica√ß√£o:")
    print(classification_report(all_labels, all_predictions, target_names=classes))
    
    # Matriz de confus√£o
    cm = confusion_matrix(all_labels, all_predictions)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=classes, yticklabels=classes)
    plt.title('Matriz de Confus√£o')
    plt.xlabel('Predi√ß√£o')
    plt.ylabel('Real')
    plt.tight_layout()
    plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return accuracy, f1, all_predictions, all_labels

def fine_tune_model(model, train_loader, test_loader, classes, device):
    """
    Fine-tuning: descongelar algumas camadas e treinar com learning rate menor
    """
    print("üîß Iniciando fine-tuning...")
    
    # Descongelar √∫ltimas camadas do backbone
    for param in model.features[-3:].parameters():
        param.requires_grad = True
    
    # Novo otimizador com learning rate menor
    optimizer_ft = optim.Adam(model.parameters(), lr=0.0001)
    criterion = nn.CrossEntropyLoss()
    
    # Treinar por mais algumas √©pocas
    for epoch in range(3):
        model.train()
        running_loss = 0.0
        
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            
            optimizer_ft.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer_ft.step()
            
            running_loss += loss.item()
        
        print(f"Fine-tuning √âpoca [{epoch+1}/3] - Loss: {running_loss/len(train_loader):.4f}")
    
    # Avaliar ap√≥s fine-tuning
    print("\nüéØ Avalia√ß√£o ap√≥s fine-tuning:")
    accuracy_ft, f1_ft, _, _ = evaluate_model(model, test_loader, classes, device)
    
    return accuracy_ft, f1_ft

def main():
    """
    Fun√ß√£o principal que executa todo o pipeline
    """
    print("üöÄ Iniciando projeto de classifica√ß√£o de imagens")
    print("=" * 50)
    
    # Configurar device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"üíª Usando device: {device}")
    
    # Preparar dados
    train_loader, test_loader, classes = prepare_data()
    num_classes = len(classes)
    
    # Criar modelo
    model = create_model(num_classes)
    model = model.to(device)
    
    # Configurar treinamento
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    
    # Treinamento inicial
    train_losses = train_model(model, train_loader, criterion, optimizer, device)
    
    # Avalia√ß√£o inicial
    print("\n" + "="*50)
    accuracy_initial, f1_initial, _, _ = evaluate_model(model, test_loader, classes, device)
    
    # Fine-tuning
    print("\n" + "="*50)
    accuracy_final, f1_final = fine_tune_model(model, train_loader, test_loader, classes, device)
    
    # Compara√ß√£o de resultados
    print("\n" + "="*50)
    print("üìà COMPARA√á√ÉO DE RESULTADOS:")
    print(f"   Antes do fine-tuning:")
    print(f"     - Acur√°cia: {accuracy_initial:.4f}")
    print(f"     - F1-Score: {f1_initial:.4f}")
    print(f"   Ap√≥s fine-tuning:")
    print(f"     - Acur√°cia: {accuracy_final:.4f}")
    print(f"     - F1-Score: {f1_final:.4f}")
    print(f"   Melhoria:")
    print(f"     - Acur√°cia: {accuracy_final - accuracy_initial:+.4f}")
    print(f"     - F1-Score: {f1_final - f1_initial:+.4f}")
    
    # Salvar modelo
    torch.save(model.state_dict(), model_path)
    print(f"\n‚úÖ Modelo salvo como '{model_path}'")
    
    # Plotar curva de treinamento
    plt.figure(figsize=(10, 6))
    plt.plot(range(1, len(train_losses) + 1), train_losses, 'b-', label='Loss de Treinamento')
    plt.xlabel('√âpoca')
    plt.ylabel('Loss')
    plt.title('Curva de Treinamento')
    plt.legend()
    plt.grid(True)
    plt.savefig('training_curve.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print("\nüéâ Modelo finalizado com sucesso!")

if __name__ == "__main__":
    main()
